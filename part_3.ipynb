{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from part_3_config import config as cfg\n",
    "import preprocess as pp\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LEN = cfg['parameters']['train_len']['value']\n",
    "VALIDATION_LEN = cfg['parameters']['validation_len']['value']\n",
    "TEST_LEN = cfg['parameters']['test_len']['value']\n",
    "\n",
    "# TRAIN_LEN = 1000\n",
    "# VALIDATION_LEN = 100\n",
    "# TEST_LEN = 100\n",
    "\n",
    "MAX_LEN = cfg['parameters']['max_len']['value']\n",
    "\n",
    "dir = 'RES/TF/TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5351663\n",
      "30000\n",
      "10000\n",
      "12590\n",
      "oh, i don t say that your excellency will do as you please i should be sorry to advise you in the matter.\n"
     ]
    }
   ],
   "source": [
    "train_sents, validation_sents, test_sents = pp.get_sents('Auguste_Maquet.txt', TRAIN_LEN, VALIDATION_LEN, TEST_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from gensim.models import KeyedVectors\n",
    "import torch\n",
    "\n",
    "class SentencesDataset(Dataset):\n",
    "    def __init__(self, sentences: list, Emb: KeyedVectors, max_len: int = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if max_len is not None:\n",
    "            SentencesDataset.max_len = max_len + 1\n",
    "\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            s = pp.get_sentence_index(sentence, Emb)\n",
    "            max_sentence_len = min(SentencesDataset.max_len, len(s))\n",
    "\n",
    "            self.X.append(torch.cat((s[:max_sentence_len], torch.empty(SentencesDataset.max_len - max_sentence_len, dtype=torch.long).fill_(Emb.key_to_index['pad']))))\n",
    "\n",
    "            # self.Y.append(s[max_sentence_len])\n",
    "            # for i in range(max_sentence_len):\n",
    "            #     self.X.append(torch.cat((s[:i], torch.empty(SentencesDataset.max_len - i, dtype=torch.long).fill_(Emb.key_to_index['pad']))))\n",
    "            #     self.Y.append(s[i])\n",
    "\n",
    "        self.X = torch.stack(self.X)\n",
    "        # self.Y = torch.stack(self.Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return self.X[idx], self.Y[idx]\n",
    "        return self.X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(dataset, batch_size, shuffle):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(Emb, batch_size, device, max_len):\n",
    "    train_dataset = SentencesDataset(train_sents, Emb, max_len)\n",
    "    validation_dataset = SentencesDataset(validation_sents, Emb)\n",
    "    test_dataset = SentencesDataset(test_sents, Emb)\n",
    "\n",
    "    train_dataloader = get_dataloader(train_dataset, batch_size, True)\n",
    "    validation_dataloader = get_dataloader(validation_dataset, batch_size, True)\n",
    "    test_dataloader = get_dataloader(test_dataset, batch_size, False)\n",
    "\n",
    "    return train_dataloader, validation_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def run(model, dataloader, train, es, device, loss_fn, optimizer, epoch):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "\n",
    "    for X in pbar:\n",
    "        # print(X[0])\n",
    "        Y = X[:, 1:]\n",
    "        X = X[:, :-1]\n",
    "\n",
    "        # print(X[0])\n",
    "        # print(Y[0])\n",
    "\n",
    "        # break\n",
    "\n",
    "        Y_pred = model(X)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        Y_pred = Y_pred.view(-1, Y_pred.shape[-1])\n",
    "        Y = Y.view(-1)\n",
    "\n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        pbar.set_description(f'{epoch} {\"T\" if train else \"V\"} Loss: {loss.item():7.4f}, Avg Loss: {sum(epoch_loss) / len(epoch_loss):7.4f}, Best Loss: {es.best_loss:7.4f}, Counter: {es.counter}')\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def train_epoch(model, train_dataloader, validation_dataloader, es, device, loss_fn, optimizer, epoch):\n",
    "    train_loss = run(model, train_dataloader, True, es, device, loss_fn, optimizer, epoch)\n",
    "    wandb.log({'train_loss': train_loss})\n",
    "    with torch.no_grad():\n",
    "        validation_loss = run(model, validation_dataloader, False, es, device, loss_fn, optimizer, epoch)\n",
    "        wandb.log({'validation_loss': validation_loss})\n",
    "    print(f'Epoch {epoch} Train Loss: {train_loss:7.4f}, Validation Loss: {validation_loss:7.4f}')\n",
    "    return train_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Decoder\n",
    "from EarlyStopping import EarlyStopping\n",
    "import torch.nn as nn\n",
    "\n",
    "def train(train_dataloader, validation_dataloader, cfg, Emb):\n",
    "\n",
    "    # nhead = cfg['parameters']['nhead']['value']\n",
    "    # dim_feedforward = cfg['parameters']['dim_feedforward']['value']\n",
    "    # num_layers = cfg['parameters']['num_layers']['value']\n",
    "    # dropout = cfg['parameters']['dropout']['value']\n",
    "    # max_len = cfg['parameters']['max_len']['value']\n",
    "    # epochs = cfg['parameters']['epochs']['value']\n",
    "    # learning_rate = cfg['parameters']['learning_rate']['value']\n",
    "    # optimizer = cfg['parameters']['optimizer']['value']\n",
    "\n",
    "    nhead = cfg.nhead\n",
    "    dim_feedforward = cfg.dim_feedforward\n",
    "    num_layers = cfg.num_layers\n",
    "    dropout = cfg.dropout\n",
    "    max_len = cfg.max_len\n",
    "    epochs = cfg.epochs\n",
    "    learning_rate = cfg.learning_rate\n",
    "    optimizer = cfg.optimizer\n",
    "\n",
    "    model = Decoder(Emb, nhead, dim_feedforward, num_layers, dropout, max_len, pp.device).to(pp.device)\n",
    "    # print(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = getattr(torch.optim, optimizer)(model.parameters(), lr=learning_rate)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    es = EarlyStopping(patience=3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        _, validation_loss = train_epoch(model, train_dataloader, validation_dataloader, es, pp.device, loss_fn, optimizer, epoch)\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), os.path.join(dir, f'nnlm_{epoch}.pth'))\n",
    "\n",
    "        if es(validation_loss, epoch):\n",
    "            break\n",
    "\n",
    "    os.rename(os.path.join(dir, f'nnlm_{es.best_model_pth}.pth'), os.path.join(dir, f'best_model.pth'))\n",
    "\n",
    "    return es.best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnlm import NNLM\n",
    "import tqdm\n",
    "\n",
    "def run_perplexity(dataloader, best_model, best_pth, Emb):\n",
    "    best_model.load_state_dict(torch.load(best_pth))\n",
    "    best_model.eval()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        perplexity = []\n",
    "\n",
    "        pbar = tqdm.tqdm(dataloader)\n",
    "        for X in pbar:\n",
    "            Y = X[:, 1:]\n",
    "            X = X[:, :-1]\n",
    "\n",
    "            # tgt_key_padding_mask = (X == Emb.key_to_index['pad']).transpose(0, 1)\n",
    "\n",
    "            Y_pred = best_model(X)\n",
    "            Y = Y.to(pp.device)\n",
    "\n",
    "            for i in range(Y_pred.shape[0]):\n",
    "                pval = 0\n",
    "                pix = 0\n",
    "                sentence = ''\n",
    "                \n",
    "                # print(X.shape)\n",
    "                # print(Y_pred.shape)\n",
    "                # print(Y.shape)\n",
    "\n",
    "                for j in range(1, X.shape[1]):\n",
    "                    if X[i][j].item() == Emb.key_to_index['eos']:\n",
    "                        break\n",
    "                    sentence += Emb.index_to_key[X[i][j].item()] + ' '\n",
    "                    pix = j + 1\n",
    "\n",
    "                # print(sentence.strip())\n",
    "                # print(Y_pred[i][:pix].shape)\n",
    "                # print(Y[i][:pix].shape)\n",
    "                # print(Y_pred[i][:pix])\n",
    "                # print(Y[i][:pix])\n",
    "\n",
    "                # print(loss_fn(Y_pred[i][:pix], Y[i][:pix]).item())\n",
    "\n",
    "                pval = np.exp(loss_fn(Y_pred[i][:pix], Y[i][:pix]).item())\n",
    "                # print(f'{sentence.strip()}: {pval}', file=f)\n",
    "                perplexity.append(pval)\n",
    "\n",
    "            #     break\n",
    "            # break\n",
    "\n",
    "        # print(f'Perplexity: {np.mean(perplexity)}')\n",
    "        return np.mean(perplexity)\n",
    "\n",
    "def get_all_perplexity_vals(test_dataloader, cfg, Emb):\n",
    "\n",
    "    # nhead = cfg['parameters']['nhead']['value']\n",
    "    # dim_feedforward = cfg['parameters']['dim_feedforward']['value']\n",
    "    # num_layers = cfg['parameters']['num_layers']['value']\n",
    "    # dropout = cfg['parameters']['dropout']['value']\n",
    "    # max_len = cfg['parameters']['max_len']['value']\n",
    "\n",
    "    nhead = cfg.nhead\n",
    "    dim_feedforward = cfg.dim_feedforward\n",
    "    num_layers = cfg.num_layers\n",
    "    dropout = cfg.dropout\n",
    "    max_len = cfg.max_len\n",
    "\n",
    "    best_model = Decoder(Emb, nhead, dim_feedforward, num_layers, dropout, max_len, pp.device).to(pp.device)\n",
    "    # print(best_model)\n",
    "    best_pth = os.path.join(dir, 'best_model.pth')\n",
    "\n",
    "    # with open(os.path.join(dir, 'test_perplexity.txt'), 'w') as f:\n",
    "    return run_perplexity(test_dataloader, best_model, best_pth, Emb)\n",
    "    \n",
    "    # return run_perplexity(test_dataloader, best_model, best_pth, Emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 6t5kvtay\n",
      "Sweep URL: https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fd41icek with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_feedforward: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: CrossEntropyLoss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_len: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnhead: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_len: 14000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_len: 30000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_len: 10000\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshu7bh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/shu7bh/ANLP/Assignments/1/wandb/run-20230903_025458-fd41icek</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shu7bh/Transformer/runs/fd41icek' target=\"_blank\">lemon-sweep-1</a></strong> to <a href='https://wandb.ai/shu7bh/Transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shu7bh/Transformer' target=\"_blank\">https://wandb.ai/shu7bh/Transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shu7bh/Transformer/runs/fd41icek' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/runs/fd41icek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16244\n",
      "Decoder(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=200, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=200, bias=True)\n",
      "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "        (dropout3): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=200, out_features=16244, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 T Loss:  1.8468, Avg Loss:  2.1582, Best Loss:     inf, Counter: 0: 100%|██████████| 938/938 [00:20<00:00, 45.84it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "0 V Loss:  2.4069, Avg Loss:  1.9215, Best Loss:     inf, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 86.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss:  2.1582, Validation Loss:  1.9215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 T Loss:  1.6935, Avg Loss:  1.8503, Best Loss:  1.9215, Counter: 0: 100%|██████████| 938/938 [00:19<00:00, 48.82it/s]\n",
      "1 V Loss:  1.6556, Avg Loss:  1.8337, Best Loss:  1.9215, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 91.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss:  1.8503, Validation Loss:  1.8337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2 T Loss:  1.9591, Avg Loss:  1.7365, Best Loss:  1.8337, Counter: 0: 100%|██████████| 938/938 [00:18<00:00, 49.47it/s]\n",
      "2 V Loss:  1.4286, Avg Loss:  1.7962, Best Loss:  1.8337, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 87.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss:  1.7365, Validation Loss:  1.7962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 T Loss:  1.4524, Avg Loss:  1.6504, Best Loss:  1.7962, Counter: 0: 100%|██████████| 938/938 [00:19<00:00, 48.01it/s]\n",
      "3 V Loss:  2.1360, Avg Loss:  1.7968, Best Loss:  1.7962, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 86.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss:  1.6504, Validation Loss:  1.7968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4 T Loss:  1.1378, Avg Loss:  1.5763, Best Loss:  1.7962, Counter: 1: 100%|██████████| 938/938 [00:18<00:00, 50.05it/s]\n",
      "4 V Loss:  1.4021, Avg Loss:  1.7990, Best Loss:  1.7962, Counter: 1: 100%|██████████| 313/313 [00:03<00:00, 82.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss:  1.5763, Validation Loss:  1.7990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5 T Loss:  1.1499, Avg Loss:  1.5119, Best Loss:  1.7962, Counter: 2: 100%|██████████| 938/938 [00:18<00:00, 49.89it/s]\n",
      "5 V Loss:  1.6905, Avg Loss:  1.8095, Best Loss:  1.7962, Counter: 2: 100%|██████████| 313/313 [00:03<00:00, 89.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss:  1.5119, Validation Loss:  1.8095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6 T Loss:  1.3726, Avg Loss:  1.4562, Best Loss:  1.7962, Counter: 3: 100%|██████████| 938/938 [00:19<00:00, 49.29it/s]\n",
      "6 V Loss:  1.6323, Avg Loss:  1.8323, Best Loss:  1.7962, Counter: 3: 100%|██████████| 313/313 [00:03<00:00, 86.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss:  1.4562, Validation Loss:  1.8323\n",
      "Decoder(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=200, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=200, bias=True)\n",
      "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "        (dropout3): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=200, out_features=16244, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 42.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=200, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=200, bias=True)\n",
      "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "        (dropout3): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=200, out_features=16244, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [00:09<00:00, 42.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_loss</td><td>▁</td></tr><tr><td>test_perplexity</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁</td></tr><tr><td>train_perplexity</td><td>▁</td></tr><tr><td>validation_loss</td><td>█▃▁▁▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_loss</td><td>1.79625</td></tr><tr><td>test_perplexity</td><td>90.12837</td></tr><tr><td>train_loss</td><td>1.45622</td></tr><tr><td>train_perplexity</td><td>52.88513</td></tr><tr><td>validation_loss</td><td>1.83234</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-sweep-1</strong> at: <a href='https://wandb.ai/shu7bh/Transformer/runs/fd41icek' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/runs/fd41icek</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230903_025458-fd41icek/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zrhlds67 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_feedforward: 2048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: CrossEntropyLoss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_len: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnhead: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_len: 14000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_len: 30000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_len: 10000\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/shu7bh/ANLP/Assignments/1/wandb/run-20230903_025926-zrhlds67</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shu7bh/Transformer/runs/zrhlds67' target=\"_blank\">vibrant-sweep-2</a></strong> to <a href='https://wandb.ai/shu7bh/Transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shu7bh/Transformer' target=\"_blank\">https://wandb.ai/shu7bh/Transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shu7bh/Transformer/runs/zrhlds67' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/runs/zrhlds67</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16244\n",
      "Decoder(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.4, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
      "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "        (dropout3): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=200, out_features=16244, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 T Loss:  1.6567, Avg Loss:  2.2546, Best Loss:     inf, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.99it/s]\n",
      "  0%|          | 0/625 [00:00<?, ?it/s]/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "0 V Loss:  2.1350, Avg Loss:  2.0256, Best Loss:     inf, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 110.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss:  2.2546, Validation Loss:  2.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 T Loss:  2.2766, Avg Loss:  2.0359, Best Loss:  2.0256, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 49.26it/s]\n",
      "1 V Loss:  1.7964, Avg Loss:  1.9598, Best Loss:  2.0256, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 110.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss:  2.0359, Validation Loss:  1.9598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2 T Loss:  1.7111, Avg Loss:  1.9797, Best Loss:  1.9598, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.66it/s]\n",
      "2 V Loss:  2.0192, Avg Loss:  1.9298, Best Loss:  1.9598, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 111.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss:  1.9797, Validation Loss:  1.9298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 T Loss:  2.3892, Avg Loss:  1.9457, Best Loss:  1.9298, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.81it/s]\n",
      "3 V Loss:  2.3045, Avg Loss:  1.9160, Best Loss:  1.9298, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 106.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss:  1.9457, Validation Loss:  1.9160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4 T Loss:  1.6891, Avg Loss:  1.9219, Best Loss:  1.9160, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 49.22it/s]\n",
      "4 V Loss:  1.5383, Avg Loss:  1.9079, Best Loss:  1.9160, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 106.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss:  1.9219, Validation Loss:  1.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5 T Loss:  1.8266, Avg Loss:  1.9036, Best Loss:  1.9079, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.57it/s]\n",
      "5 V Loss:  1.2864, Avg Loss:  1.8956, Best Loss:  1.9079, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 104.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss:  1.9036, Validation Loss:  1.8956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6 T Loss:  2.1637, Avg Loss:  1.8893, Best Loss:  1.8956, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.22it/s]\n",
      "6 V Loss:  1.9505, Avg Loss:  1.8940, Best Loss:  1.8956, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 106.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss:  1.8893, Validation Loss:  1.8940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7 T Loss:  2.1301, Avg Loss:  1.8769, Best Loss:  1.8940, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.45it/s]\n",
      "7 V Loss:  2.1939, Avg Loss:  1.8923, Best Loss:  1.8940, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 104.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Loss:  1.8769, Validation Loss:  1.8923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8 T Loss:  1.8120, Avg Loss:  1.8660, Best Loss:  1.8923, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.62it/s]\n",
      "8 V Loss:  2.0343, Avg Loss:  1.8870, Best Loss:  1.8923, Counter: 0: 100%|██████████| 625/625 [00:06<00:00, 103.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train Loss:  1.8660, Validation Loss:  1.8870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9 T Loss:  1.7911, Avg Loss:  1.8573, Best Loss:  1.8870, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.52it/s]\n",
      "9 V Loss:  1.9339, Avg Loss:  1.8847, Best Loss:  1.8870, Counter: 0: 100%|██████████| 625/625 [00:06<00:00, 101.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train Loss:  1.8573, Validation Loss:  1.8847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 T Loss:  2.6289, Avg Loss:  1.8501, Best Loss:  1.8847, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.16it/s]\n",
      "10 V Loss:  1.6245, Avg Loss:  1.8813, Best Loss:  1.8847, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 107.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train Loss:  1.8501, Validation Loss:  1.8813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 T Loss:  2.5782, Avg Loss:  1.8429, Best Loss:  1.8813, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.99it/s]\n",
      "11 V Loss:  2.1850, Avg Loss:  1.8836, Best Loss:  1.8813, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 108.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train Loss:  1.8429, Validation Loss:  1.8836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12 T Loss:  2.6148, Avg Loss:  1.8370, Best Loss:  1.8813, Counter: 1: 100%|██████████| 1875/1875 [00:38<00:00, 48.25it/s]\n",
      "12 V Loss:  1.6484, Avg Loss:  1.8809, Best Loss:  1.8813, Counter: 1: 100%|██████████| 625/625 [00:05<00:00, 108.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train Loss:  1.8370, Validation Loss:  1.8809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13 T Loss:  1.7679, Avg Loss:  1.8314, Best Loss:  1.8809, Counter: 0: 100%|██████████| 1875/1875 [00:38<00:00, 48.60it/s]\n",
      "13 V Loss:  1.8018, Avg Loss:  1.8837, Best Loss:  1.8809, Counter: 0: 100%|██████████| 625/625 [00:05<00:00, 105.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train Loss:  1.8314, Validation Loss:  1.8837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14 T Loss:  1.9653, Avg Loss:  1.8258, Best Loss:  1.8809, Counter: 1: 100%|██████████| 1875/1875 [00:38<00:00, 48.71it/s]\n",
      "14 V Loss:  2.3570, Avg Loss:  1.8854, Best Loss:  1.8809, Counter: 1: 100%|██████████| 625/625 [00:05<00:00, 106.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train Loss:  1.8258, Validation Loss:  1.8854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15 T Loss:  1.8534, Avg Loss:  1.8215, Best Loss:  1.8809, Counter: 2: 100%|██████████| 1875/1875 [00:38<00:00, 48.23it/s]\n",
      "15 V Loss:  1.4732, Avg Loss:  1.8821, Best Loss:  1.8809, Counter: 2: 100%|██████████| 625/625 [00:06<00:00, 101.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train Loss:  1.8215, Validation Loss:  1.8821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16 T Loss:  2.1245, Avg Loss:  1.8171, Best Loss:  1.8809, Counter: 3: 100%|██████████| 1875/1875 [00:38<00:00, 48.31it/s]\n",
      "16 V Loss:  2.5289, Avg Loss:  1.8726, Best Loss:  1.8809, Counter: 3: 100%|██████████| 625/625 [00:07<00:00, 84.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train Loss:  1.8171, Validation Loss:  1.8726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17 T Loss:  2.4266, Avg Loss:  1.8124, Best Loss:  1.8726, Counter: 0: 100%|██████████| 1875/1875 [00:40<00:00, 46.03it/s]\n",
      "17 V Loss:  2.4828, Avg Loss:  1.8819, Best Loss:  1.8726, Counter: 0: 100%|██████████| 625/625 [00:06<00:00, 96.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Train Loss:  1.8124, Validation Loss:  1.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18 T Loss:  1.3710, Avg Loss:  1.8095, Best Loss:  1.8726, Counter: 1: 100%|██████████| 1875/1875 [00:41<00:00, 45.52it/s]\n",
      "18 V Loss:  1.9615, Avg Loss:  1.8817, Best Loss:  1.8726, Counter: 1: 100%|██████████| 625/625 [00:06<00:00, 97.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Train Loss:  1.8095, Validation Loss:  1.8817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19 T Loss:  1.5485, Avg Loss:  1.8059, Best Loss:  1.8726, Counter: 2: 100%|██████████| 1875/1875 [00:40<00:00, 46.81it/s]\n",
      "19 V Loss:  1.6692, Avg Loss:  1.8805, Best Loss:  1.8726, Counter: 2: 100%|██████████| 625/625 [00:06<00:00, 103.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Train Loss:  1.8059, Validation Loss:  1.8805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20 T Loss:  2.0064, Avg Loss:  1.8025, Best Loss:  1.8726, Counter: 3: 100%|██████████| 1875/1875 [00:38<00:00, 48.13it/s]\n",
      "20 V Loss:  1.5193, Avg Loss:  1.8841, Best Loss:  1.8726, Counter: 3: 100%|██████████| 625/625 [00:05<00:00, 105.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Train Loss:  1.8025, Validation Loss:  1.8841\n",
      "Decoder(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.4, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
      "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "        (dropout3): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=200, out_features=16244, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:26<00:00, 71.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.4, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
      "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "        (dropout3): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=200, out_features=16244, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [00:11<00:00, 68.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_loss</td><td>▁</td></tr><tr><td>test_perplexity</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_perplexity</td><td>▁</td></tr><tr><td>validation_loss</td><td>█▅▄▃▃▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_loss</td><td>1.87263</td></tr><tr><td>test_perplexity</td><td>122.70458</td></tr><tr><td>train_loss</td><td>1.80247</td></tr><tr><td>train_perplexity</td><td>63.41482</td></tr><tr><td>validation_loss</td><td>1.88406</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-sweep-2</strong> at: <a href='https://wandb.ai/shu7bh/Transformer/runs/zrhlds67' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/runs/zrhlds67</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230903_025926-zrhlds67/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ajz2f6n2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_feedforward: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: CrossEntropyLoss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_len: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnhead: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_len: 14000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_len: 30000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_len: 10000\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/shu7bh/ANLP/Assignments/1/wandb/run-20230903_031709-ajz2f6n2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shu7bh/Transformer/runs/ajz2f6n2' target=\"_blank\">mild-sweep-3</a></strong> to <a href='https://wandb.ai/shu7bh/Transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shu7bh/Transformer' target=\"_blank\">https://wandb.ai/shu7bh/Transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/sweeps/6t5kvtay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shu7bh/Transformer/runs/ajz2f6n2' target=\"_blank\">https://wandb.ai/shu7bh/Transformer/runs/ajz2f6n2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=100, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=100, bias=True)\n",
      "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "        (dropout3): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=100, out_features=16244, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 T Loss:  2.1434, Avg Loss:  3.2927, Best Loss:     inf, Counter: 0:   4%|▎         | 34/938 [00:01<00:21, 41.68it/s]Exception ignored in: <generator object tqdm.__iter__ at 0x7f3d54f12c30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/tqdm/std.py\", line 1193, in __iter__\n",
      "    self.close()\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/tqdm/std.py\", line 1287, in close\n",
      "    fp_write('')\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/tqdm/std.py\", line 1284, in fp_write\n",
      "    self.fp.write(str(s))\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/tqdm/utils.py\", line 127, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/ipykernel/iostream.py\", line 559, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/ipykernel/iostream.py\", line 251, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/zmq/sugar/socket.py\", line 618, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "          zmq.error.ZMQError: Socket operation on non-socket\n"
     ]
    }
   ],
   "source": [
    "# WANDB init\n",
    "import wandb\n",
    "\n",
    "def run_everything(cfg=None):\n",
    "    with wandb.init(config=cfg):\n",
    "        cfg = wandb.config\n",
    "\n",
    "        # embedding_dim = cfg['parameters']['embedding_dim']['value']\n",
    "        # batch_size = cfg['parameters']['batch_size']['value']\n",
    "        # max_len = cfg['parameters']['max_len']['value']\n",
    "\n",
    "        embedding_dim = cfg.embedding_dim\n",
    "        batch_size = cfg.batch_size\n",
    "        max_len = cfg.max_len\n",
    "\n",
    "        Emb = pp.create_vocab(train_sents, embedding_dim)\n",
    "        print(len(Emb.key_to_index))\n",
    "\n",
    "        train_dataloader, validation_dataloader, test_dataloader = load_data(Emb, batch_size, pp.device, max_len)\n",
    "\n",
    "        val_loss = train(train_dataloader, validation_dataloader, cfg, Emb)\n",
    "        wandb.log({'best_loss': val_loss})\n",
    "\n",
    "        train_perplexity = get_all_perplexity_vals(train_dataloader, cfg, Emb)\n",
    "        test_perplexity = get_all_perplexity_vals(test_dataloader, cfg, Emb)\n",
    "\n",
    "        wandb.log({'train_perplexity': train_perplexity})\n",
    "        wandb.log({'test_perplexity': test_perplexity})\n",
    "\n",
    "sweep_id = wandb.sweep(cfg, project=\"Transformer\")\n",
    "wandb.agent(sweep_id, run_everything, count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshu7bh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/shu7bh/ANLP/Assignments/1/wandb/run-20230903_023612-emi71204</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shu7bh/TEST/runs/emi71204' target=\"_blank\">whole-flower-16</a></strong> to <a href='https://wandb.ai/shu7bh/TEST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shu7bh/TEST' target=\"_blank\">https://wandb.ai/shu7bh/TEST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shu7bh/TEST/runs/emi71204' target=\"_blank\">https://wandb.ai/shu7bh/TEST/runs/emi71204</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16244\n",
      "Decoder(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=100, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=100, bias=True)\n",
      "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=100, out_features=16244, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 T Loss:  1.8565, Avg Loss:  2.2712, Best Loss:     inf, Counter: 0: 100%|██████████| 938/938 [00:21<00:00, 42.79it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "0 V Loss:  2.1339, Avg Loss:  1.9637, Best Loss:     inf, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 84.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss:  2.2712, Validation Loss:  1.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 T Loss:  1.6589, Avg Loss:  1.9537, Best Loss:  1.9637, Counter: 0: 100%|██████████| 938/938 [00:20<00:00, 44.82it/s]\n",
      "1 V Loss:  2.4867, Avg Loss:  1.8854, Best Loss:  1.9637, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 87.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss:  1.9537, Validation Loss:  1.8854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2 T Loss:  1.7063, Avg Loss:  1.8783, Best Loss:  1.8854, Counter: 0: 100%|██████████| 938/938 [00:21<00:00, 44.65it/s]\n",
      "2 V Loss:  2.2040, Avg Loss:  1.8460, Best Loss:  1.8854, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 87.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss:  1.8783, Validation Loss:  1.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 T Loss:  1.4080, Avg Loss:  1.8269, Best Loss:  1.8460, Counter: 0: 100%|██████████| 938/938 [00:21<00:00, 44.16it/s]\n",
      "3 V Loss:  1.6040, Avg Loss:  1.8170, Best Loss:  1.8460, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 83.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss:  1.8269, Validation Loss:  1.8170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4 T Loss:  1.5742, Avg Loss:  1.7869, Best Loss:  1.8170, Counter: 0: 100%|██████████| 938/938 [00:21<00:00, 44.66it/s]\n",
      "4 V Loss:  1.5240, Avg Loss:  1.8022, Best Loss:  1.8170, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 88.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss:  1.7869, Validation Loss:  1.8022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5 T Loss:  2.0394, Avg Loss:  1.7555, Best Loss:  1.8022, Counter: 0: 100%|██████████| 938/938 [00:21<00:00, 43.43it/s]\n",
      "5 V Loss:  1.6512, Avg Loss:  1.7921, Best Loss:  1.8022, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 87.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss:  1.7555, Validation Loss:  1.7921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6 T Loss:  1.7619, Avg Loss:  1.7283, Best Loss:  1.7921, Counter: 0: 100%|██████████| 938/938 [00:20<00:00, 45.01it/s]\n",
      "6 V Loss:  1.7325, Avg Loss:  1.7899, Best Loss:  1.7921, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 89.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss:  1.7283, Validation Loss:  1.7899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7 T Loss:  1.6470, Avg Loss:  1.7046, Best Loss:  1.7899, Counter: 0: 100%|██████████| 938/938 [00:21<00:00, 44.46it/s]\n",
      "7 V Loss:  1.9572, Avg Loss:  1.7870, Best Loss:  1.7899, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 93.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Loss:  1.7046, Validation Loss:  1.7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8 T Loss:  1.7151, Avg Loss:  1.6844, Best Loss:  1.7870, Counter: 0: 100%|██████████| 938/938 [00:21<00:00, 44.61it/s]\n",
      "8 V Loss:  1.9002, Avg Loss:  1.7872, Best Loss:  1.7870, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 86.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train Loss:  1.6844, Validation Loss:  1.7872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9 T Loss:  1.4786, Avg Loss:  1.6678, Best Loss:  1.7870, Counter: 1: 100%|██████████| 938/938 [00:20<00:00, 45.20it/s]\n",
      "9 V Loss:  2.3662, Avg Loss:  1.7888, Best Loss:  1.7870, Counter: 1: 100%|██████████| 313/313 [00:03<00:00, 92.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train Loss:  1.6678, Validation Loss:  1.7888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 T Loss:  2.1462, Avg Loss:  1.6521, Best Loss:  1.7870, Counter: 2: 100%|██████████| 938/938 [00:20<00:00, 44.83it/s]\n",
      "10 V Loss:  1.4500, Avg Loss:  1.7863, Best Loss:  1.7870, Counter: 2: 100%|██████████| 313/313 [00:03<00:00, 89.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train Loss:  1.6521, Validation Loss:  1.7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 T Loss:  1.3926, Avg Loss:  1.6385, Best Loss:  1.7863, Counter: 0: 100%|██████████| 938/938 [00:20<00:00, 44.77it/s]\n",
      "11 V Loss:  1.6273, Avg Loss:  1.7903, Best Loss:  1.7863, Counter: 0: 100%|██████████| 313/313 [00:03<00:00, 91.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train Loss:  1.6385, Validation Loss:  1.7903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12 T Loss:  2.0145, Avg Loss:  1.6270, Best Loss:  1.7863, Counter: 1: 100%|██████████| 938/938 [00:20<00:00, 44.83it/s]\n",
      "12 V Loss:  2.3817, Avg Loss:  1.7931, Best Loss:  1.7863, Counter: 1: 100%|██████████| 313/313 [00:03<00:00, 91.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train Loss:  1.6270, Validation Loss:  1.7931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13 T Loss:  1.8187, Avg Loss:  1.6173, Best Loss:  1.7863, Counter: 2: 100%|██████████| 938/938 [00:21<00:00, 44.53it/s]\n",
      "13 V Loss:  1.6690, Avg Loss:  1.7945, Best Loss:  1.7863, Counter: 2: 100%|██████████| 313/313 [00:03<00:00, 92.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train Loss:  1.6173, Validation Loss:  1.7945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14 T Loss:  1.2294, Avg Loss:  1.6061, Best Loss:  1.7863, Counter: 3: 100%|██████████| 938/938 [00:20<00:00, 45.60it/s]\n",
      "14 V Loss:  1.6619, Avg Loss:  1.7976, Best Loss:  1.7863, Counter: 3: 100%|██████████| 313/313 [00:03<00:00, 84.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train Loss:  1.6061, Validation Loss:  1.7976\n",
      "Decoder(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=100, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=100, bias=True)\n",
      "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=100, out_features=16244, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [00:10<00:00, 38.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_loss</td><td>▁</td></tr><tr><td>test_perplexity</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_loss</td><td>1.78628</td></tr><tr><td>test_perplexity</td><td>98.46982</td></tr><tr><td>train_loss</td><td>1.60607</td></tr><tr><td>validation_loss</td><td>1.79763</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whole-flower-16</strong> at: <a href='https://wandb.ai/shu7bh/TEST/runs/emi71204' target=\"_blank\">https://wandb.ai/shu7bh/TEST/runs/emi71204</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230903_023612-emi71204/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run_everything(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from part_3_config import config as cfg\n",
    "# import preprocess as pp\n",
    "\n",
    "# embedding_dim = cfg['parameters']['embedding_dim']['value']\n",
    "# batch_size = cfg['parameters']['batch_size']['value']\n",
    "# max_len = cfg['parameters']['max_len']['value']\n",
    "\n",
    "# Emb = pp.create_vocab(train_sents, embedding_dim)\n",
    "# train_dataloader, validation_dataloader, test_dataloader = load_data(Emb, batch_size, pp.device, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_perplexity_vals(test_dataloader, cfg, Emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nhead = cfg['parameters']['nhead']['value']\n",
    "# dim_feedforward = cfg['parameters']['dim_feedforward']['value']\n",
    "# num_layers = cfg['parameters']['num_layers']['value']\n",
    "# dropout = cfg['parameters']['dropout']['value']\n",
    "# max_len = cfg['parameters']['max_len']['value']\n",
    "\n",
    "# best_model = Decoder(Emb, nhead, dim_feedforward, num_layers, dropout, max_len, pp.device).to(pp.device)\n",
    "# print(best_model)\n",
    "# best_pth = os.path.join(dir, 'best_model.pth')\n",
    "\n",
    "# # generate a sentence from the model\n",
    "\n",
    "# q = 'my name is '\n",
    "# print(q, end=' ')\n",
    "\n",
    "# best_model.load_state_dict(torch.load(best_pth))\n",
    "# best_model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in range(10):\n",
    "#         e = pp.get_sentence_index(q, Emb)\n",
    "#         e = e[:-1]\n",
    "#         # print(e)\n",
    "#         X = torch.cat((e, torch.empty(max_len - len(e), dtype=torch.long).fill_(Emb.key_to_index['pad'])))\n",
    "#         # print (X)\n",
    "#         Y_pred = best_model(X)\n",
    "#         # print(Y_pred.shape)\n",
    "#         Y_pred = Y_pred[0][len(e) - 1]\n",
    "#         Y_pred = torch.softmax(Y_pred, dim=-1)\n",
    "#         # Y_pred = torch.multinomial(Y_pred, num_samples=1)\n",
    "#         Y_pred = torch.argmax(Y_pred)\n",
    "\n",
    "\n",
    "#         # print(Y_pred.shape)\n",
    "\n",
    "#         # for j in range(len(e)):\n",
    "#         #     a = torch.softmax(Y_pred[0, j], dim=0)\n",
    "#         #     # b = torch.multinomial(a, num_samples=1)\n",
    "#         #     b = torch.argmax(a)\n",
    "#         #     print(Emb.index_to_key[e[j].item()], Emb.index_to_key[b.item()])\n",
    "#         # print (Y_pred)\n",
    "#         # Y_pred = Y_pred[0, len(e) - 1]\n",
    "#         # # Y_pred[len(e)]\n",
    "#         # Y_pred = torch.softmax(Y_pred, dim=-1)\n",
    "#         # print(Y_pred)\n",
    "#         # Y_pred = torch.multinomial(Y_pred, num_samples=1)\n",
    "\n",
    "#         q += ' ' + Emb.index_to_key[Y_pred.item()]\n",
    "#         # print the word\n",
    "#         print(Emb.index_to_key[Y_pred.item()], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
