{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5351663\n",
      "30000\n",
      "10000\n",
      "12590\n",
      "and what will that signify to me?\n",
      "16204\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import preprocess as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'RES/LSTM/TEST/200'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Creating a custom dataset which has one variable: data\n",
    "\n",
    "data contains a list of sentences where each sentence is a list of words' indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentencesDataset(Dataset):\n",
    "    def __init__(self, sentences: list, Emb):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = []\n",
    "        for sentence in sentences:\n",
    "            self.data.append(pp.get_sentence_index(sentence, Emb))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = SentencesDataset(pp.train_sents, pp.Emb)\n",
    "validation_data = SentencesDataset(pp.validation_sents, pp.Emb)\n",
    "test_data = SentencesDataset(pp.test_sents, pp.Emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom collate function which pads the sentences to the same length with the max length of the batch.\n",
    "\n",
    "This helps in parallelizing calling the LSTM by stacking the sentences of the same length together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_collate(X, Emb):\n",
    "    # get max length in X\n",
    "    max_len = max(map(lambda x: len(x), X))\n",
    "    # set the pred tensor to be of the same size as the X\n",
    "    Y = []\n",
    "    for i in range(len(X)):\n",
    "        # get the device of the tensor\n",
    "        X[i] = torch.cat((X[i], torch.empty(max_len - len(X[i]), dtype=torch.long).fill_(Emb.key_to_index['pad'])))\n",
    "        Y.append(X[i][1:])\n",
    "        X[i] = X[i][:-1]\n",
    "    return torch.stack(X), torch.stack(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataloaders for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def wrapper_collate(batch):\n",
    "    return padding_collate(batch, pp.Emb)\n",
    "\n",
    "training_dataloader = DataLoader(training_data, batch_size=pp.batch_size, shuffle=True, collate_fn=wrapper_collate)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=pp.batch_size, shuffle=True, collate_fn=wrapper_collate)\n",
    "test_dataloader = DataLoader(test_data, batch_size=pp.batch_size, shuffle=False, collate_fn=wrapper_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model, optimizer, and the loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM\n",
    "\n",
    "lstm = LSTM(pp.Emb, pp.hidden_dim, pp.dropout, pp.device).to(pp.device)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=pp.learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16204"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pp.Emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One run of the dataloader is defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def run(lstm, dataloader, train, es):\n",
    "    if train:\n",
    "        lstm.train()\n",
    "    else:\n",
    "        lstm.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "\n",
    "    for X, Y in pbar:\n",
    "        lstm.init_hidden()\n",
    "        Y_pred = []\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            Y_pred.append(lstm(X[:, i]))\n",
    "\n",
    "        Y_pred = torch.stack(Y_pred, dim=1)\n",
    "        Y_pred = Y_pred.view(-1, Y_pred.shape[2])\n",
    "        Y = Y.view(-1).to(pp.device)\n",
    "\n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        pbar.set_description(f'{\"T\" if train else \"V\"} Loss: {loss.item():7.4f}, Avg Loss: {np.mean(epoch_loss):7.4f}, Best Loss: {es.best_loss:7.4f}, Counter: {es.counter}')\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model here\n",
    "The best weights are saved as best_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.3167, Avg Loss:  1.7340, Best Loss:     inf, Counter: 0: 100%|██████████| 235/235 [01:20<00:00,  2.91it/s]\n",
      "V Loss:  1.9274, Avg Loss:  1.3626, Best Loss:     inf, Counter: 0: 100%|██████████| 79/79 [00:12<00:00,  6.16it/s]\n",
      "T Loss:  1.7123, Avg Loss:  1.2450, Best Loss:  1.3626, Counter: 0: 100%|██████████| 235/235 [01:22<00:00,  2.85it/s]\n",
      "V Loss:  2.2248, Avg Loss:  1.2248, Best Loss:  1.3626, Counter: 0: 100%|██████████| 79/79 [00:13<00:00,  5.95it/s]\n",
      "T Loss:  1.4928, Avg Loss:  1.2004, Best Loss:  1.2248, Counter: 0: 100%|██████████| 235/235 [01:20<00:00,  2.92it/s]\n",
      "V Loss:  2.6012, Avg Loss:  1.1915, Best Loss:  1.2248, Counter: 0: 100%|██████████| 79/79 [00:12<00:00,  6.25it/s]\n",
      "T Loss:  0.8197, Avg Loss:  1.1372, Best Loss:  1.1915, Counter: 0: 100%|██████████| 235/235 [01:21<00:00,  2.88it/s]\n",
      "V Loss:  2.6255, Avg Loss:  1.1361, Best Loss:  1.1915, Counter: 0: 100%|██████████| 79/79 [00:12<00:00,  6.17it/s]\n",
      "T Loss:  1.4928, Avg Loss:  1.1203, Best Loss:  1.1361, Counter: 0: 100%|██████████| 235/235 [01:22<00:00,  2.86it/s]\n",
      "V Loss:  2.2043, Avg Loss:  1.1430, Best Loss:  1.1361, Counter: 0: 100%|██████████| 79/79 [00:12<00:00,  6.24it/s]\n",
      "T Loss:  1.1521, Avg Loss:  1.0822, Best Loss:  1.1361, Counter: 1: 100%|██████████| 235/235 [01:21<00:00,  2.87it/s]\n",
      "V Loss:  2.1301, Avg Loss:  1.1113, Best Loss:  1.1361, Counter: 1: 100%|██████████| 79/79 [00:12<00:00,  6.08it/s]\n",
      "T Loss:  1.6345, Avg Loss:  1.0709, Best Loss:  1.1113, Counter: 0: 100%|██████████| 235/235 [01:23<00:00,  2.82it/s]\n",
      "V Loss:  1.1562, Avg Loss:  1.0945, Best Loss:  1.1113, Counter: 0: 100%|██████████| 79/79 [00:12<00:00,  6.23it/s]\n",
      "T Loss:  0.8511, Avg Loss:  1.0474, Best Loss:  1.0945, Counter: 0: 100%|██████████| 235/235 [01:22<00:00,  2.83it/s]\n",
      "V Loss:  1.3181, Avg Loss:  1.0489, Best Loss:  1.0945, Counter: 0: 100%|██████████| 79/79 [00:12<00:00,  6.22it/s]\n",
      "T Loss:  1.5876, Avg Loss:  1.0327, Best Loss:  1.0489, Counter: 0: 100%|██████████| 235/235 [01:21<00:00,  2.87it/s]\n",
      "V Loss:  1.8188, Avg Loss:  1.0487, Best Loss:  1.0489, Counter: 0: 100%|██████████| 79/79 [00:13<00:00,  6.06it/s]\n",
      "T Loss:  1.1623, Avg Loss:  1.0092, Best Loss:  1.0487, Counter: 0: 100%|██████████| 235/235 [01:21<00:00,  2.88it/s]\n",
      "V Loss:  2.1272, Avg Loss:  1.0970, Best Loss:  1.0487, Counter: 0: 100%|██████████| 79/79 [00:12<00:00,  6.33it/s]\n",
      "T Loss:  1.5049, Avg Loss:  1.0083, Best Loss:  1.0487, Counter: 1: 100%|██████████| 235/235 [01:21<00:00,  2.90it/s]\n",
      "V Loss:  2.0916, Avg Loss:  1.0298, Best Loss:  1.0487, Counter: 1: 100%|██████████| 79/79 [00:12<00:00,  6.10it/s]\n",
      "T Loss:  1.9045, Avg Loss:  0.9911, Best Loss:  1.0298, Counter: 0: 100%|██████████| 235/235 [01:34<00:00,  2.49it/s]\n",
      "V Loss:  1.9477, Avg Loss:  1.0311, Best Loss:  1.0298, Counter: 0: 100%|██████████| 79/79 [00:15<00:00,  5.07it/s]\n",
      "T Loss:  0.9311, Avg Loss:  0.9883, Best Loss:  1.0298, Counter: 1: 100%|██████████| 235/235 [01:22<00:00,  2.86it/s]\n",
      "V Loss:  1.8729, Avg Loss:  1.0464, Best Loss:  1.0298, Counter: 1: 100%|██████████| 79/79 [00:13<00:00,  5.90it/s]\n",
      "T Loss:  0.9259, Avg Loss:  0.9644, Best Loss:  1.0298, Counter: 2: 100%|██████████| 235/235 [01:24<00:00,  2.77it/s]\n",
      "V Loss:  1.4454, Avg Loss:  1.0292, Best Loss:  1.0298, Counter: 2: 100%|██████████| 79/79 [00:12<00:00,  6.16it/s]\n",
      "T Loss:  0.8605, Avg Loss:  0.9550, Best Loss:  1.0292, Counter: 0: 100%|██████████| 235/235 [01:20<00:00,  2.92it/s]\n",
      "V Loss:  1.6072, Avg Loss:  1.0334, Best Loss:  1.0292, Counter: 0: 100%|██████████| 79/79 [00:12<00:00,  6.22it/s]\n",
      "T Loss:  1.2758, Avg Loss:  0.9490, Best Loss:  1.0292, Counter: 1: 100%|██████████| 235/235 [01:22<00:00,  2.85it/s]\n",
      "V Loss:  1.8578, Avg Loss:  1.0151, Best Loss:  1.0292, Counter: 1: 100%|██████████| 79/79 [00:14<00:00,  5.50it/s]\n",
      "T Loss:  0.7473, Avg Loss:  0.9345, Best Loss:  1.0151, Counter: 0: 100%|██████████| 235/235 [01:25<00:00,  2.73it/s]\n",
      "V Loss:  1.4216, Avg Loss:  1.0303, Best Loss:  1.0151, Counter: 0: 100%|██████████| 79/79 [00:13<00:00,  6.03it/s]\n",
      "T Loss:  1.2760, Avg Loss:  0.9240, Best Loss:  1.0151, Counter: 1: 100%|██████████| 235/235 [01:22<00:00,  2.84it/s]\n",
      "V Loss:  1.4842, Avg Loss:  1.0402, Best Loss:  1.0151, Counter: 1: 100%|██████████| 79/79 [00:12<00:00,  6.24it/s]\n",
      "T Loss:  1.1534, Avg Loss:  0.9199, Best Loss:  1.0151, Counter: 2: 100%|██████████| 235/235 [01:22<00:00,  2.86it/s]\n",
      "V Loss:  2.1983, Avg Loss:  1.0159, Best Loss:  1.0151, Counter: 2: 100%|██████████| 79/79 [00:12<00:00,  6.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import EarlyStopping as ES\n",
    "\n",
    "es = ES.EarlyStopping()\n",
    "\n",
    "for epoch in range(pp.epochs):\n",
    "    print(f'Epoch {epoch+1}' + '\\n')\n",
    "\n",
    "    epoch_loss = run(lstm, training_dataloader, True, es)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = run(lstm, validation_dataloader, False, es)\n",
    "        if es(epoch_loss, epoch):\n",
    "            break\n",
    "\n",
    "    torch.save(lstm.state_dict(), os.path.join(dir, f'lstm_{epoch + 1}.pth'))\n",
    "\n",
    "os.rename(os.path.join(dir, f'lstm_{es.best_model_pth + 1}.pth'), os.path.join(dir, 'best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = LSTM(pp.Emb, pp.hidden_dim, pp.dropout, pp.device).to(pp.device)\n",
    "best_pth = os.path.join(dir, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the perplexity scores for each sentence and outputting to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# test\n",
    "def run_perplexity(dataloader, f):\n",
    "    # f = sys.stdout\n",
    "    best_model.load_state_dict(torch.load(best_pth))\n",
    "    best_model.eval()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        perplexity = []\n",
    "\n",
    "        for X, Y in tqdm.tqdm(dataloader):\n",
    "            best_model.init_hidden()\n",
    "            Y_pred = []\n",
    "\n",
    "            for i in range(X.shape[1]):\n",
    "                Y_pred.append(best_model(X[:, i]))\n",
    "\n",
    "            Y_pred = torch.stack(Y_pred, dim=1).to(pp.device)\n",
    "\n",
    "            for i in range(Y_pred.shape[0]):\n",
    "                sentence = ''\n",
    "                for j in range(Y.shape[1]):\n",
    "                    if Y[i][j] == pp.Emb.key_to_index['eos']:\n",
    "                        Y_pred_ = Y_pred[i][:j]\n",
    "                        Y_ = Y[i][:j].to(pp.device)\n",
    "                        loss = loss_fn(Y_pred_, Y_)\n",
    "                        perplexity.append(torch.exp(loss).item())\n",
    "                        sentence = sentence.strip()\n",
    "                        print(f'{sentence}: {perplexity[-1]}', file=f)\n",
    "                        break\n",
    "                    else:\n",
    "                        sentence += pp.Emb.index_to_key[Y[i][j].item()] + ' '\n",
    "\n",
    "        print(f'Average Perplexity: {np.mean(perplexity)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:00<00:00,  3.86it/s]\n",
      "100%|██████████| 79/79 [00:20<00:00,  3.82it/s]\n",
      "100%|██████████| 99/99 [00:25<00:00,  3.81it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(dir, 'train.txt'), 'w') as f:\n",
    "    run_perplexity(train_dataloader, f)\n",
    "\n",
    "with open(os.path.join(dir, 'validation.txt'), 'w') as f:\n",
    "    run_perplexity(val_dataloader, f)\n",
    "\n",
    "with open(os.path.join(dir, 'test.txt'), 'w') as f:\n",
    "    run_perplexity(test_dataloader, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General trial of how the model works. Problem in this is that I am taking the multinomial distribution for the next word and not the argmax. This is because the argmax will always give the same word and the model will not be able to generate new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "montmartre monsieur him travelling the profoundest of a person posted that the bandit , when at length he had lost his eyes sink she emptied on the table , yielding or folded , which the other disappeared had first dark of the luminous metamorphosis by admiringly intently , a kiss , calling awaiting that she wished felt pity . eos "
     ]
    }
   ],
   "source": [
    "# predict a sentence\n",
    "\n",
    "best_model.load_state_dict(torch.load(best_pth))\n",
    "best_model.eval()\n",
    "\n",
    "current_word = 'this'\n",
    "best_model.init_hidden()\n",
    "\n",
    "while current_word != 'eos':\n",
    "    X = pp.get_vocab_index(current_word, pp.Emb)\n",
    "\n",
    "    Y_pred = best_model(X)\n",
    "    # multinomial distribution on y_pred to get the next word\n",
    "    Y_pred = torch.multinomial(torch.softmax(Y_pred, dim=0), 1).item()\n",
    "    # Y_pred = torch.argmax(Y_pred, dim=0).item()\n",
    "\n",
    "    current_word = pp.Emb.index_to_key[Y_pred]\n",
    "    print(current_word, end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
