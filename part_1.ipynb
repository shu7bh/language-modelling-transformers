{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import preprocess as pp\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = config['parameters']['train_len']['value']\n",
    "validation_len = config['parameters']['validation_len']['value']\n",
    "test_len = config['parameters']['test_len']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5351663\n",
      "30000\n",
      "10000\n",
      "12590\n",
      "and what will that signify to me?\n"
     ]
    }
   ],
   "source": [
    "train_sents, validation_sents, test_sents = pp.get_sents('Auguste_Maquet.txt', train_len, validation_len, test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir = 'RES/NNLM/TEST'\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentencesDataset(Dataset):\n",
    "    def __init__(self, sentences: list, Emb):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            s = pp.get_sentence_index_pad(sentence, Emb)\n",
    "\n",
    "            for i in range(5, len(s)):\n",
    "                self.X.append(s[i - 5:i])\n",
    "                self.Y.append(s[i])\n",
    "\n",
    "        self.X = torch.stack(self.X)\n",
    "        self.Y = torch.stack(self.Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(dataset, batch_size, shuffle):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(Emb, batch_size):\n",
    "    train_dataset = SentencesDataset(train_sents, Emb)\n",
    "    validation_dataset = SentencesDataset(validation_sents, Emb)\n",
    "    test_dataset = SentencesDataset(test_sents, Emb)\n",
    "\n",
    "    train_dataloader = get_dataloader(train_dataset, batch_size, shuffle=True)\n",
    "    validation_dataloader = get_dataloader(validation_dataset, batch_size, shuffle=True)\n",
    "    test_dataloader = get_dataloader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, validation_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def run(model, dataloader, train, es, device, loss_fn, optimizer, epoch):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "\n",
    "    for X, Y in pbar:\n",
    "        Y_pred = model(X)\n",
    "\n",
    "        Y = Y.to(device)\n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        pbar.set_description(f'{epoch} {\"T\" if train else \"V\"} Loss: {loss.item():7.4f}, Avg Loss: {sum(epoch_loss) / len(epoch_loss):7.4f}, Best Loss: {es.best_loss:7.4f}, Counter: {es.counter}')\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, validation_dataloader, es, device, loss_fn, optimizer, epoch):\n",
    "    train_loss = run(model, train_dataloader, True, es, device, loss_fn, optimizer, epoch)\n",
    "    with torch.no_grad():\n",
    "        validation_loss = run(model, validation_dataloader, False, es, device, loss_fn, optimizer, epoch)\n",
    "    return train_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnlm import NNLM\n",
    "from EarlyStopping import EarlyStopping\n",
    "import torch.nn as nn\n",
    "\n",
    "def train(train_dataloader, validation_dataloader, cfg, Emb):\n",
    "\n",
    "    model = NNLM(Emb, cfg.hidden_dim, cfg.dropout, pp.device).to(pp.device)\n",
    "    print(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = getattr(torch.optim, cfg.optimizer)(model.parameters(), lr=cfg.learning_rate)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "\n",
    "    es = EarlyStopping(patience=3)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        _, validation_loss = train_epoch(model, train_dataloader, validation_dataloader, es, pp.device, loss_fn, optimizer, epoch)\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), os.path.join(dir, f'nnlm_{epoch}.pth'))\n",
    "\n",
    "        if es(validation_loss, epoch):\n",
    "            break\n",
    "\n",
    "    os.rename(os.path.join(dir, f'nnlm_{es.best_model_pth}.pth'), os.path.join(dir, f'best_model.pth'))\n",
    "\n",
    "    return es.best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnlm import NNLM\n",
    "import tqdm\n",
    "\n",
    "def run_perplexity(dataloader, best_model, best_pth, Emb):\n",
    "    best_model.load_state_dict(torch.load(best_pth))\n",
    "    best_model.eval()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        perplexity = []\n",
    "\n",
    "        current_sentence = ''\n",
    "        current_pred = []\n",
    "        current_truth = []\n",
    "\n",
    "        for X, Y in tqdm.tqdm(dataloader):\n",
    "            Y_pred = best_model(X)\n",
    "\n",
    "            for i in range(Y.shape[0]):\n",
    "                if Y[i].item() == Emb.key_to_index['eos']:\n",
    "                    if len(current_pred) == 0:\n",
    "                        continue\n",
    "\n",
    "                    current_pred = torch.stack(current_pred).to(pp.device)\n",
    "                    current_truth = torch.tensor(current_truth).to(pp.device)\n",
    "                    loss = loss_fn(current_pred, current_truth)\n",
    "\n",
    "                    if torch.exp(loss).item() < 10000:\n",
    "                        perplexity.append(torch.exp(loss).item())\n",
    "\n",
    "                    current_sentence = ''\n",
    "                    current_pred = []\n",
    "                    current_truth = []\n",
    "\n",
    "                elif Y[i].item() == Emb.key_to_index['pad'] or Y[i].item() == Emb.key_to_index['sos']:\n",
    "                    continue\n",
    "                else:\n",
    "                    current_sentence += Emb.index_to_key[Y[i].item()] + ' '\n",
    "                    current_pred.append(Y_pred[i])\n",
    "                    current_truth.append(Y[i])\n",
    "\n",
    "        print(f'Perplexity: {np.mean(perplexity)}')\n",
    "        return np.mean(perplexity)\n",
    "\n",
    "def get_all_perplexity_vals(test_dataloader, cfg, Emb):\n",
    "    best_model = NNLM(Emb, cfg.hidden_dim, cfg.dropout, pp.device).to(pp.device)\n",
    "    best_pth = os.path.join(dir, 'best_model.pth')\n",
    "\n",
    "    return run_perplexity(test_dataloader, best_model, best_pth, Emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 2165b26r\n",
      "Sweep URL: https://wandb.ai/shu7bh/nnlm/sweeps/2165b26r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tp7k1y9s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_len: 14000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_len: 30000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_len: 10000\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshu7bh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/shu7bh/ANLP/Assignments/1/wandb/run-20230901_175213-tp7k1y9s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shu7bh/nnlm/runs/tp7k1y9s' target=\"_blank\">super-sweep-1</a></strong> to <a href='https://wandb.ai/shu7bh/nnlm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shu7bh/nnlm/sweeps/2165b26r' target=\"_blank\">https://wandb.ai/shu7bh/nnlm/sweeps/2165b26r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shu7bh/nnlm' target=\"_blank\">https://wandb.ai/shu7bh/nnlm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shu7bh/nnlm/sweeps/2165b26r' target=\"_blank\">https://wandb.ai/shu7bh/nnlm/sweeps/2165b26r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shu7bh/nnlm/runs/tp7k1y9s' target=\"_blank\">https://wandb.ai/shu7bh/nnlm/runs/tp7k1y9s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16204\n",
      "NNLM(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=16204, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.2156, Avg Loss:  7.9760, Best Loss:     inf, Counter: 0: 100%|██████████| 5049/5049 [00:28<00:00, 178.73it/s]\n",
      "V Loss:  6.9858, Avg Loss:  6.8834, Best Loss:     inf, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.55it/s]\n",
      "T Loss:  6.3389, Avg Loss:  6.6452, Best Loss:  6.8834, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.80it/s]\n",
      "V Loss:  5.4691, Avg Loss:  6.4146, Best Loss:  6.8834, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 212.00it/s]\n",
      "T Loss:  6.5523, Avg Loss:  6.3263, Best Loss:  6.4146, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 184.05it/s]\n",
      "V Loss:  6.0220, Avg Loss:  6.1911, Best Loss:  6.4146, Counter: 0: 100%|██████████| 1699/1699 [00:07<00:00, 213.63it/s]\n",
      "T Loss:  5.9810, Avg Loss:  6.1500, Best Loss:  6.1911, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.29it/s]\n",
      "V Loss:  6.2944, Avg Loss:  6.0516, Best Loss:  6.1911, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.55it/s]\n",
      "T Loss:  5.9182, Avg Loss:  6.0332, Best Loss:  6.0516, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.19it/s]\n",
      "V Loss:  5.4563, Avg Loss:  5.9546, Best Loss:  6.0516, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.83it/s]\n",
      "T Loss:  5.9158, Avg Loss:  5.9493, Best Loss:  5.9546, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.76it/s]\n",
      "V Loss:  6.0308, Avg Loss:  5.8829, Best Loss:  5.9546, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 211.41it/s]\n",
      "T Loss:  5.9411, Avg Loss:  5.8848, Best Loss:  5.8829, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.35it/s]\n",
      "V Loss:  5.7164, Avg Loss:  5.8252, Best Loss:  5.8829, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.77it/s]\n",
      "T Loss:  6.3723, Avg Loss:  5.8320, Best Loss:  5.8252, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.38it/s]\n",
      "V Loss:  5.3194, Avg Loss:  5.7782, Best Loss:  5.8252, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.85it/s]\n",
      "T Loss:  5.9347, Avg Loss:  5.7873, Best Loss:  5.7782, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.96it/s]\n",
      "V Loss:  6.0087, Avg Loss:  5.7380, Best Loss:  5.7782, Counter: 0: 100%|██████████| 1699/1699 [00:07<00:00, 212.80it/s]\n",
      "T Loss:  5.0335, Avg Loss:  5.7486, Best Loss:  5.7380, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.01it/s]\n",
      "V Loss:  6.3237, Avg Loss:  5.7025, Best Loss:  5.7380, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 212.15it/s]\n",
      "T Loss:  5.7761, Avg Loss:  5.7148, Best Loss:  5.7025, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.39it/s]\n",
      "V Loss:  5.6390, Avg Loss:  5.6713, Best Loss:  5.7025, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.35it/s]\n",
      "T Loss:  5.1900, Avg Loss:  5.6846, Best Loss:  5.6713, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.15it/s]\n",
      "V Loss:  6.6829, Avg Loss:  5.6443, Best Loss:  5.6713, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.94it/s]\n",
      "T Loss:  5.4301, Avg Loss:  5.6574, Best Loss:  5.6443, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.38it/s]\n",
      "V Loss:  5.6221, Avg Loss:  5.6185, Best Loss:  5.6443, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.84it/s]\n",
      "T Loss:  5.8675, Avg Loss:  5.6326, Best Loss:  5.6185, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.40it/s]\n",
      "V Loss:  5.7326, Avg Loss:  5.5959, Best Loss:  5.6185, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 207.87it/s]\n",
      "T Loss:  5.1702, Avg Loss:  5.6096, Best Loss:  5.5959, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.54it/s]\n",
      "V Loss:  6.0195, Avg Loss:  5.5750, Best Loss:  5.5959, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.44it/s]\n",
      "T Loss:  6.7049, Avg Loss:  5.5885, Best Loss:  5.5750, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.89it/s]\n",
      "V Loss:  5.6515, Avg Loss:  5.5537, Best Loss:  5.5750, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.50it/s]\n",
      "T Loss:  5.0214, Avg Loss:  5.5684, Best Loss:  5.5537, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.62it/s]\n",
      "V Loss:  5.6925, Avg Loss:  5.5344, Best Loss:  5.5537, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.82it/s]\n",
      "T Loss:  5.3068, Avg Loss:  5.5496, Best Loss:  5.5344, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.38it/s]\n",
      "V Loss:  6.0006, Avg Loss:  5.5175, Best Loss:  5.5344, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 211.55it/s]\n",
      "T Loss:  5.5612, Avg Loss:  5.5319, Best Loss:  5.5175, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.04it/s]\n",
      "V Loss:  5.8629, Avg Loss:  5.4999, Best Loss:  5.5175, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.34it/s]\n",
      "T Loss:  5.4459, Avg Loss:  5.5150, Best Loss:  5.4999, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.62it/s]\n",
      "V Loss:  5.5719, Avg Loss:  5.4845, Best Loss:  5.4999, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 212.22it/s]\n",
      "T Loss:  5.7124, Avg Loss:  5.4990, Best Loss:  5.4845, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.48it/s]\n",
      "V Loss:  5.3436, Avg Loss:  5.4679, Best Loss:  5.4845, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.28it/s]\n",
      "T Loss:  5.5197, Avg Loss:  5.4837, Best Loss:  5.4679, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.07it/s]\n",
      "V Loss:  5.6702, Avg Loss:  5.4535, Best Loss:  5.4679, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 211.05it/s]\n",
      "T Loss:  5.6628, Avg Loss:  5.4690, Best Loss:  5.4535, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.28it/s]\n",
      "V Loss:  5.2244, Avg Loss:  5.4398, Best Loss:  5.4535, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 207.05it/s]\n",
      "T Loss:  4.9260, Avg Loss:  5.4550, Best Loss:  5.4398, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.19it/s]\n",
      "V Loss:  5.6195, Avg Loss:  5.4267, Best Loss:  5.4398, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.73it/s]\n",
      "T Loss:  5.4394, Avg Loss:  5.4416, Best Loss:  5.4267, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.91it/s]\n",
      "V Loss:  5.7631, Avg Loss:  5.4133, Best Loss:  5.4267, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.30it/s]\n",
      "T Loss:  4.8137, Avg Loss:  5.4285, Best Loss:  5.4133, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.20it/s]\n",
      "V Loss:  6.2009, Avg Loss:  5.4017, Best Loss:  5.4133, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.22it/s]\n",
      "T Loss:  4.8176, Avg Loss:  5.4161, Best Loss:  5.4017, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.03it/s]\n",
      "V Loss:  5.2432, Avg Loss:  5.3906, Best Loss:  5.4017, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 205.85it/s]\n",
      "T Loss:  5.4009, Avg Loss:  5.4043, Best Loss:  5.3906, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.28it/s]\n",
      "V Loss:  5.2263, Avg Loss:  5.3777, Best Loss:  5.3906, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.61it/s]\n",
      "T Loss:  5.4380, Avg Loss:  5.3928, Best Loss:  5.3777, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.57it/s]\n",
      "V Loss:  5.3095, Avg Loss:  5.3676, Best Loss:  5.3777, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.64it/s]\n",
      "T Loss:  5.3446, Avg Loss:  5.3816, Best Loss:  5.3676, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.02it/s]\n",
      "V Loss:  5.1573, Avg Loss:  5.3570, Best Loss:  5.3676, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.91it/s]\n",
      "T Loss:  5.9038, Avg Loss:  5.3709, Best Loss:  5.3570, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.04it/s]\n",
      "V Loss:  5.2386, Avg Loss:  5.3460, Best Loss:  5.3570, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.24it/s]\n",
      "T Loss:  5.3676, Avg Loss:  5.3605, Best Loss:  5.3460, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 181.84it/s]\n",
      "V Loss:  5.0610, Avg Loss:  5.3355, Best Loss:  5.3460, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.07it/s]\n",
      "T Loss:  6.0067, Avg Loss:  5.3505, Best Loss:  5.3355, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.50it/s]\n",
      "V Loss:  5.5758, Avg Loss:  5.3275, Best Loss:  5.3355, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.68it/s]\n",
      "T Loss:  6.5131, Avg Loss:  5.3408, Best Loss:  5.3275, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.71it/s]\n",
      "V Loss:  5.6822, Avg Loss:  5.3183, Best Loss:  5.3275, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 206.36it/s]\n",
      "T Loss:  5.0736, Avg Loss:  5.3312, Best Loss:  5.3183, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.97it/s]\n",
      "V Loss:  5.4956, Avg Loss:  5.3092, Best Loss:  5.3183, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.73it/s]\n",
      "T Loss:  5.6022, Avg Loss:  5.3222, Best Loss:  5.3092, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.68it/s]\n",
      "V Loss:  4.2712, Avg Loss:  5.2999, Best Loss:  5.3092, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.63it/s]\n",
      "T Loss:  5.3370, Avg Loss:  5.3132, Best Loss:  5.2999, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.51it/s]\n",
      "V Loss:  4.3547, Avg Loss:  5.2917, Best Loss:  5.2999, Counter: 0: 100%|██████████| 1699/1699 [00:07<00:00, 212.73it/s]\n",
      "T Loss:  4.9036, Avg Loss:  5.3046, Best Loss:  5.2917, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.71it/s]\n",
      "V Loss:  5.4790, Avg Loss:  5.2836, Best Loss:  5.2917, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.75it/s]\n",
      "T Loss:  5.7196, Avg Loss:  5.2963, Best Loss:  5.2836, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.71it/s]\n",
      "V Loss:  6.1739, Avg Loss:  5.2754, Best Loss:  5.2836, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 207.57it/s]\n",
      "T Loss:  5.2262, Avg Loss:  5.2881, Best Loss:  5.2754, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.88it/s]\n",
      "V Loss:  5.5154, Avg Loss:  5.2680, Best Loss:  5.2754, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.80it/s]\n",
      "T Loss:  5.7281, Avg Loss:  5.2801, Best Loss:  5.2680, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.82it/s]\n",
      "V Loss:  5.7000, Avg Loss:  5.2606, Best Loss:  5.2680, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 211.53it/s]\n",
      "T Loss:  5.0635, Avg Loss:  5.2722, Best Loss:  5.2606, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 181.41it/s]\n",
      "V Loss:  5.5559, Avg Loss:  5.2530, Best Loss:  5.2606, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.70it/s]\n",
      "T Loss:  4.6836, Avg Loss:  5.2647, Best Loss:  5.2530, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.25it/s]\n",
      "V Loss:  5.8665, Avg Loss:  5.2453, Best Loss:  5.2530, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.22it/s]\n",
      "T Loss:  5.1046, Avg Loss:  5.2573, Best Loss:  5.2453, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.87it/s]\n",
      "V Loss:  4.9982, Avg Loss:  5.2399, Best Loss:  5.2453, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.74it/s]\n",
      "T Loss:  4.9293, Avg Loss:  5.2501, Best Loss:  5.2399, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 181.87it/s]\n",
      "V Loss:  4.9259, Avg Loss:  5.2322, Best Loss:  5.2399, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.99it/s]\n",
      "T Loss:  4.7680, Avg Loss:  5.2430, Best Loss:  5.2322, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.81it/s]\n",
      "V Loss:  4.1503, Avg Loss:  5.2257, Best Loss:  5.2322, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 207.62it/s]\n",
      "T Loss:  4.8930, Avg Loss:  5.2362, Best Loss:  5.2257, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.51it/s]\n",
      "V Loss:  4.8473, Avg Loss:  5.2195, Best Loss:  5.2257, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.46it/s]\n",
      "T Loss:  5.4494, Avg Loss:  5.2295, Best Loss:  5.2195, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 181.35it/s]\n",
      "V Loss:  4.9291, Avg Loss:  5.2137, Best Loss:  5.2195, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.00it/s]\n",
      "T Loss:  4.9662, Avg Loss:  5.2228, Best Loss:  5.2137, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.16it/s]\n",
      "V Loss:  5.5019, Avg Loss:  5.2076, Best Loss:  5.2137, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.26it/s]\n",
      "T Loss:  5.2507, Avg Loss:  5.2165, Best Loss:  5.2076, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.98it/s]\n",
      "V Loss:  4.7675, Avg Loss:  5.2015, Best Loss:  5.2076, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.80it/s]\n",
      "T Loss:  5.4008, Avg Loss:  5.2101, Best Loss:  5.2015, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 181.68it/s]\n",
      "V Loss:  5.3007, Avg Loss:  5.1952, Best Loss:  5.2015, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.13it/s]\n",
      "T Loss:  5.5433, Avg Loss:  5.2039, Best Loss:  5.1952, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 183.23it/s]\n",
      "V Loss:  5.1283, Avg Loss:  5.1894, Best Loss:  5.1952, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 207.43it/s]\n",
      "T Loss:  5.1794, Avg Loss:  5.1977, Best Loss:  5.1894, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.07it/s]\n",
      "V Loss:  5.0096, Avg Loss:  5.1836, Best Loss:  5.1894, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 205.33it/s]\n",
      "T Loss:  5.8758, Avg Loss:  5.1919, Best Loss:  5.1836, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.07it/s]\n",
      "V Loss:  5.0647, Avg Loss:  5.1786, Best Loss:  5.1836, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.62it/s]\n",
      "T Loss:  4.7591, Avg Loss:  5.1859, Best Loss:  5.1786, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.09it/s]\n",
      "V Loss:  5.2037, Avg Loss:  5.1726, Best Loss:  5.1786, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 211.88it/s]\n",
      "T Loss:  4.8022, Avg Loss:  5.1802, Best Loss:  5.1726, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.22it/s]\n",
      "V Loss:  4.4440, Avg Loss:  5.1672, Best Loss:  5.1726, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 210.27it/s]\n",
      "T Loss:  5.1304, Avg Loss:  5.1745, Best Loss:  5.1672, Counter: 0: 100%|██████████| 5049/5049 [00:28<00:00, 179.71it/s]\n",
      "V Loss:  5.5243, Avg Loss:  5.1618, Best Loss:  5.1672, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.90it/s]\n",
      "T Loss:  5.8799, Avg Loss:  5.1691, Best Loss:  5.1618, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.91it/s]\n",
      "V Loss:  5.1664, Avg Loss:  5.1565, Best Loss:  5.1618, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.65it/s]\n",
      "T Loss:  4.9155, Avg Loss:  5.1636, Best Loss:  5.1565, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 181.81it/s]\n",
      "V Loss:  4.7908, Avg Loss:  5.1522, Best Loss:  5.1565, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.45it/s]\n",
      "T Loss:  5.6723, Avg Loss:  5.1582, Best Loss:  5.1522, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 180.75it/s]\n",
      "V Loss:  5.3832, Avg Loss:  5.1485, Best Loss:  5.1522, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.64it/s]\n",
      "T Loss:  5.1554, Avg Loss:  5.1529, Best Loss:  5.1485, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.54it/s]\n",
      "V Loss:  4.1832, Avg Loss:  5.1418, Best Loss:  5.1485, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 207.33it/s]\n",
      "T Loss:  5.4413, Avg Loss:  5.1478, Best Loss:  5.1418, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.08it/s]\n",
      "V Loss:  5.1407, Avg Loss:  5.1374, Best Loss:  5.1418, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 208.90it/s]\n",
      "T Loss:  5.5035, Avg Loss:  5.1427, Best Loss:  5.1374, Counter: 0: 100%|██████████| 5049/5049 [00:28<00:00, 180.02it/s]\n",
      "V Loss:  5.2260, Avg Loss:  5.1330, Best Loss:  5.1374, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.42it/s]\n",
      "T Loss:  4.8990, Avg Loss:  5.1376, Best Loss:  5.1330, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.27it/s]\n",
      "V Loss:  5.4436, Avg Loss:  5.1281, Best Loss:  5.1330, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 209.09it/s]\n",
      "T Loss:  5.1304, Avg Loss:  5.1326, Best Loss:  5.1281, Counter: 0: 100%|██████████| 5049/5049 [00:27<00:00, 182.25it/s]\n",
      "V Loss:  4.5706, Avg Loss:  5.1238, Best Loss:  5.1281, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 206.60it/s]\n",
      "T Loss:  4.3965, Avg Loss:  5.1277, Best Loss:  5.1238, Counter: 0: 100%|██████████| 5049/5049 [00:28<00:00, 179.27it/s]\n",
      "V Loss:  5.1991, Avg Loss:  5.1196, Best Loss:  5.1238, Counter: 0: 100%|██████████| 1699/1699 [00:08<00:00, 207.74it/s]\n",
      "T Loss:  5.2050, Avg Loss:  5.1236, Best Loss:  5.1196, Counter: 0:  77%|███████▋  | 3881/5049 [00:21<00:06, 179.60it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
      "T Loss:  4.6067, Avg Loss:  5.1234, Best Loss:  5.1196, Counter: 0:  77%|███████▋  | 3881/5049 [00:21<00:06, 179.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  4.9016, Avg Loss:  5.1228, Best Loss:  5.1196, Counter: 0:  93%|█████████▎| 4674/5049 [00:25<00:02, 182.73it/s]Exception ignored in: <generator object tqdm.__iter__ at 0x7f42e1d1ab00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/tqdm/std.py\", line 1193, in __iter__\n",
      "    self.close()\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/tqdm/std.py\", line 1287, in close\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/ipykernel/iostream.py\", line 559, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/ipykernel/iostream.py\", line 251, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home2/shu7bh/miniconda3/envs/main/lib/python3.11/site-packages/zmq/sugar/socket.py\", line 618, in send\n",
      "      File \"zmq/backend/cython/socket.pyx\", line 740, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 781, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 137, in zmq.backend.cython.socket._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n"
     ]
    }
   ],
   "source": [
    "# WANDB init\n",
    "import wandb\n",
    "\n",
    "def run_everything(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        cfg = wandb.config\n",
    "        Emb = pp.create_vocab(train_sents, cfg.embedding_dim)\n",
    "        print(len(Emb.key_to_index))\n",
    "\n",
    "        train_dataloader, validation_dataloader, test_dataloader = load_data(Emb, cfg.batch_size)\n",
    "\n",
    "        val_loss = train(train_dataloader, validation_dataloader, cfg, Emb)\n",
    "        wandb.log({'val_loss': val_loss})\n",
    "\n",
    "        train_perplexity = get_all_perplexity_vals(train_dataloader, cfg, Emb)\n",
    "        test_perplexity = get_all_perplexity_vals(test_dataloader, cfg, Emb)\n",
    "\n",
    "        wandb.log({'train_perplexity': train_perplexity})\n",
    "        wandb.log({'test_perplexity': test_perplexity})\n",
    "\n",
    "        return val_loss, test_perplexity\n",
    "    \n",
    "sweep_id = wandb.sweep(config, project=\"Assignment_1\")\n",
    "wandb.agent(sweep_id, run_everything, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nnlm import NNLM\n",
    "\n",
    "# best_model = NNLM(Emb, cfg.hidden_dim, cfg.dropout, pp.device).to(pp.device)\n",
    "# best_pth = os.path.join(dir, 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# # test\n",
    "# def run_perplexity(dataloader, f):\n",
    "#     best_model.load_state_dict(torch.load(best_pth))\n",
    "#     best_model.eval()\n",
    "\n",
    "#     loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # epoch_loss = []\n",
    "#         perplexity = []\n",
    "\n",
    "#         current_sentence = ''\n",
    "#         current_pred = []\n",
    "#         current_truth = []\n",
    "\n",
    "#         for X, Y in tqdm.tqdm(dataloader):\n",
    "#             Y_pred = best_model(X)\n",
    "\n",
    "#             for i in range(Y.shape[0]):\n",
    "#                 if Y[i].item() == Emb.key_to_index['eos']:\n",
    "#                     if len(current_pred) == 0:\n",
    "#                         continue\n",
    "\n",
    "#                     current_pred = torch.stack(current_pred).to(pp.device)\n",
    "#                     current_truth = torch.tensor(current_truth).to(pp.device)\n",
    "#                     loss = loss_fn(current_pred, current_truth)\n",
    "\n",
    "#                     if torch.exp(loss).item() < 10000:\n",
    "#                         perplexity.append(torch.exp(loss).item())\n",
    "#                         print(f'{current_sentence.strip()}: {perplexity[-1]}', file=f)\n",
    "\n",
    "#                     current_sentence = ''\n",
    "#                     current_pred = []\n",
    "#                     current_truth = []\n",
    "\n",
    "#                 elif Y[i].item() == Emb.key_to_index['pad'] or Y[i].item() == Emb.key_to_index['sos']:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     current_sentence += Emb.index_to_key[Y[i].item()] + ' '\n",
    "#                     current_pred.append(Y_pred[i])\n",
    "#                     current_truth.append(Y[i])\n",
    "\n",
    "#         print(f'Average Perplexity: {np.mean(perplexity)}', file=f)\n",
    "#         print(f'Average Perplexity: {np.mean(perplexity)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(dir, 'train.txt'), 'w') as f:\n",
    "#     run_perplexity(train_dataloader, f)\n",
    "\n",
    "# with open(os.path.join(dir, 'val.txt'), 'w') as f:\n",
    "#     run_perplexity(validation_dataloader, f)\n",
    "\n",
    "# with open(os.path.join(dir, 'test.txt'), 'w') as f:\n",
    "#     run_perplexity(test_dataloader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.load_state_dict(torch.load(best_pth))\n",
    "# with torch.no_grad():\n",
    "#     best_model.eval()\n",
    "#     query = ['money', 'is', 'the', 'root', 'of']\n",
    "#     print(*query, sep=' ', end=' ')\n",
    "\n",
    "#     X = []\n",
    "#     for word in query:\n",
    "#         X.append(get_vocab_index(word))\n",
    "\n",
    "#     while query[-1] != 'eos':\n",
    "#         Y_pred = best_model(X)\n",
    "\n",
    "#         # multinomial sampling\n",
    "#         Y_pred = torch.multinomial(torch.softmax(Y_pred, dim=1), num_samples=1)\n",
    "\n",
    "#         # Y_pred = torch.argmax(Y_pred, dim=1)\n",
    "#         query = query[1:] + [Emb.index_to_key[Y_pred[-1].item()]]\n",
    "#         X = X[1:] + [Y_pred[-1].item()]\n",
    "#         print(Emb.index_to_key[Y_pred[-1].item()], end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
